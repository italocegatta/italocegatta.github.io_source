---
title: "Web scraping: dados de páginas da internet na palma da sua mão"
date: 2017-06-16
author: Ítalo Cegatta
tags:
  - dplyr
  - leaflet
categories:
  - Web scraping
thumbnailImage: http://i.imgur.com/M5r4L5G.png
coverImage: http://i.imgur.com/UwMQdpT.png
coverCaption:
thumbnailImagePosition: top
metaAlignment: center
coverMeta: out
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE, message = FALSE,
  fig.width = 10, fig.height = 8, dpi = 400
)
```

Você já precisou copiar na mão uma informação de texto, valor ou tabela de uma pagina web? Pelo menos no meu trabalho isso é comum. Por mais que os dados estejam lá site, eles nunca estão disponíveis todos juntos e no formato que queremos. Diante disto, o objetivo deste post é mostrar como podemos utilizar o R para coletar dados de uma página web e esquecer o famooooso *ctl-c/ctl-v*.

Vamos exemplificar o post utilizando o site do IBGE para saber quantos métros cúbicos de lenha de eucalipto foram produzidos em 2015 em cada estado brasileiro. De cara, se os dados não estiverem numa tabela pronta, você já espera ter que entrar em 27 páginas diferentes para pegar esta informação.

Nosso ponto de partida é o grupo de paginas [States@](http://www.ibge.gov.br/estadosat/), que reúne várias informações dos estados brasileiros. Acessando a página podemos ver o código html por trás (utilize a tecla F12) e entender como a página está estruturada. Como queremos entrar nos estados, podemos ver na Figura \@ref(fig:pg1) que essa informação está abaixo do `id="menu"`. Note que ao passarmos o mouse sobre a linha `<div id="menu">` o navegador identifica na página a localização do elemento e ainda nos informa o id css de rastreio, no caso `div#menu`.

```{r pg1, echo=FALSE, fig.cap="Página inicial do site."}
knitr::include_graphics("http://i.imgur.com/VjiEvCM.png")
```

Então já podemos começar a programar e desenhar o acesso aos dados. No R, cada pagina web é um objeto que precisa ser salvo na memória. Então, cada página é importante por ter os dados ou ser uma etapa para consegir os dados. A página inicial (pg_raiz) contém os links para as páginas dos estados, por isso precisamos acessá-la.

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(readr, purrr, dplyr, tidyr, stringr, rvest, ggplot2, viridis, scales, sf)
pacman::p_load_gh("italocegatta/brmap")
```

```{r}
url_raiz <- "http://www.ibge.gov.br/estadosat/"

pg_raiz <- read_html(url_raiz)

posfixo_estados <- html_node(pg_raiz, "div#menu") %>% 
  html_children() %>%
  html_node("a") %>% 
  html_attr("href")

posfixo_estados
```

No objeto `posfixo_estados` temos a parte do link que leva até a página de cada estado. Para ter o link completo, é só juntar com o link da página raiz. Vamos agora acessar às páginas de todos os estados e armazenar no objeto `pg_estados`.

```{r}
url_estados <- paste0(url_raiz, posfixo_estados)

pg_estados <- map(url_estados, read_html)
```

Navegando pela página de um estado qualquer, identificamos que queremos a informação contida no link *Extração Vegetal e Silvicultura 2015*. Nesse caso, precisamos mais uma vez dos link que leva a esta página (para cada estado). 

```{r}
posfixo_lenha <- map(
  pg_estados,
  ~html_node(.x, "table.temas") %>% 
    html_children() %>%
    '['(68) %>%
    html_node("a") %>% 
    html_attr("href")
  ) %>% 
  flatten_chr()

posfixo_lenha
```

Será preciso juntar o link especifico de cada estado com a página raiz, como foi feito anteriormente. Assim, podemos acessar a pagina que tem a informação que queremos

```{r}
url_lenha <- paste0(url_raiz, posfixo_lenha)

pg_lenha <- map(url_lenha, read_html)
```

Agora, vamos dar um passo para trás e listar o nome dos estados na ordem que as páginas são acessadas.

```{r}
lista_estados <- html_node(pg_raiz, "div#menu") %>% 
  html_children() %>%
  html_node("img") %>%
  html_attr("alt")

lista_estados
```

O proximo passa é extrair a tabela de informação de cada estado e posteriormente filtar a informação que é de nosso intresse.

```{r}
tabelas <- map(
  set_names(pg_lenha, lista_estados), 
  ~html_node(.x, "table#tabela_temas") %>% 
  html_table() %>% 
  as_tibble() %>% 
  rename(produto = X1, valor = X2, unidade = X3)
  )

qnt <- map_df(
  tabelas, 
  ~filter(.x, str_detect(produto, c("Lenha de eucalipto", "quantidade"))) %>% 
  '[['("valor") 
  ) %>%
  gather(estado, volume) %>% 
  mutate(volume = as.numeric(str_replace_all(volume, "\\.", "")))

qnt 
```

De certa forma já resolvemos o problema, a quantidade de lenha de eucalipto produzida em cada estado no ano de 2015 já está em nossas mãos. Mas vamos dar um passo além e visualizar isso num mapa. O pacote [brmap](https://github.com/italocegatta/brmap) possui os polígonos dos estados brasileiros no formato `sf`, a nova classe de objetos espaciais do R.

```{r}
qnt_mapa <- left_join(estado, qnt)

ggplot(qnt_mapa) +
  geom_sf(aes(fill = volume)) +
  scale_fill_viridis(
    Lenha~de~eucalipto~(m^3), 
    na.value = "grey90",
    labels = function(x) format(x, big.mark = ".", decimal.mark = ",", scientific = FALSE)
  ) +
  theme_bw() +
  theme(legend.position = "bottom") +
  guides(fill = guide_colorbar(barwidth = 30, title.position = "top"))
```



Caso tenha alguma dúvida ou sugestão sobre o post, fique à vontade para fazer um comentário ou me contactar por Email.

```{r}
devtools::session_info()
```
